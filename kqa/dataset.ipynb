{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_json = '../dataset/kb.json'\n",
    "\n",
    "train_json = '../dataset/train.json'\n",
    "val_json = '../dataset/val.json'\n",
    "test_json = '../dataset/test.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_clean(s: str) -> str:\n",
    "    s = s.replace(',', ' and ')\n",
    "    s = ' '.join(s.split())\n",
    "    return s\n",
    "\n",
    "def find_name(kb, id):\n",
    "    try:\n",
    "        return kb['entities'][id]['name']\n",
    "    except:\n",
    "        try:\n",
    "            return kb['concepts'][id]['name']\n",
    "        except:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qualifier_relational_clean_fullname(kb_json, output=False, file_name='kb_q_r_clean_fullname.txt'):\n",
    "    qualifier = set()\n",
    "    kb = json.load(open(kb_json))\n",
    "    for i in kb['entities']:\n",
    "        fullname = kb['entities'][i]['name']\n",
    "        for rel_dict in kb['entities'][i]['relations']:\n",
    "            # First: add fact key, also called triple pairs\n",
    "            statement = list()\n",
    "            if rel_dict['direction'] == 'forward':\n",
    "                statement += [string_clean(fullname), string_clean(rel_dict['predicate']), string_clean(find_name(kb, rel_dict['object']))]\n",
    "            elif  rel_dict['direction'] == 'backward':\n",
    "                statement += [string_clean(find_name(kb, rel_dict['object'])), string_clean(rel_dict['predicate']), string_clean(fullname)]\n",
    "\n",
    "            for qk, qvs in rel_dict['qualifiers'].items():                \n",
    "                # Second add qk - qv pairs, for qv that have more than one instance, seperate to single qk - qv pairs\n",
    "                new_qvs = []\n",
    "                for qv in qvs:\n",
    "                    if qv['type'] == 'string':\n",
    "                        new_qvs.append(string_clean(qv['value']))\n",
    "                        \n",
    "                if len(new_qvs) != 0:\n",
    "                    for qv in new_qvs:\n",
    "                        statement += [string_clean(qk), qv]\n",
    "        \n",
    "            # Third: Make sure the statement is qualifier \n",
    "            if len(statement) > 3:\n",
    "                qualifier.add(tuple(statement))\n",
    "\n",
    "    # qualifier = sorted(qualifier)\n",
    "\n",
    "    if output:\n",
    "        str_q = [\",\".join(q)+'\\n' for q in qualifier]\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.writelines(str_q)\n",
    "\n",
    "    return qualifier\n",
    "\n",
    "def get_relational_clean_fullname(kb_json, output=False, file_name='kb_r_clean_fullname.txt'):\n",
    "    qualifier = set()\n",
    "    kb = json.load(open(kb_json))\n",
    "    for i in kb['entities']:\n",
    "        fullname = kb['entities'][i]['name']\n",
    "        \n",
    "        # For instance of\n",
    "        for concept_id in kb['entities'][i]['instanceOf']:\n",
    "            statement = [string_clean(fullname), 'instance of', string_clean(find_name(kb, concept_id))]\n",
    "            qualifier.add(tuple(statement))\n",
    "\n",
    "        # For relation\n",
    "        for rel_dict in kb['entities'][i]['relations']:\n",
    "            # First: add fact key, also called triple pairs\n",
    "            statement = list()\n",
    "            if rel_dict['direction'] == 'forward':\n",
    "                statement += [string_clean(fullname), string_clean(rel_dict['predicate']), string_clean(find_name(kb, rel_dict['object']))]\n",
    "            elif  rel_dict['direction'] == 'backward':\n",
    "                statement += [string_clean(find_name(kb, rel_dict['object'])), string_clean(rel_dict['predicate']), string_clean(fullname)]\n",
    "\n",
    "            for qk, qvs in rel_dict['qualifiers'].items():                \n",
    "                # Second add qk - qv pairs, for qv that have more than one instance, seperate to single qk - qv pairs\n",
    "                new_qvs = []\n",
    "                for qv in qvs:\n",
    "                    if qv['type'] == 'string':\n",
    "                        new_qvs.append(string_clean(qv['value']))\n",
    "                        \n",
    "                if len(new_qvs) != 0:\n",
    "                    for qv in new_qvs:\n",
    "                        statement += [string_clean(qk), qv]\n",
    "\n",
    "            # Third: add statement\n",
    "            qualifier.add(tuple(statement))\n",
    "\n",
    "    qualifier = sorted(qualifier)\n",
    "\n",
    "    if output:\n",
    "        str_q = [\",\".join(q)+'\\n' for q in qualifier]\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.writelines(str_q)\n",
    "    \n",
    "    return qualifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qualifier_attributes_clean_fullname(kb_json, output=False, file_name='kb_q_a_clean_fullname.txt'):\n",
    "    qualifier = set()\n",
    "    kb = json.load(open(kb_json))\n",
    "    for i in kb['entities']:\n",
    "        fullname = kb['entities'][i]['name']\n",
    "\n",
    "        # For attribute\n",
    "        for att_dict in kb['entities'][i]['attributes']:\n",
    "            # First: if it is literal, ignore it\n",
    "            if att_dict['value']['type'] != 'string':\n",
    "                continue\n",
    "            else:\n",
    "                # Second: add attributes\n",
    "                statement = list()\n",
    "                statement += [string_clean(fullname), string_clean(att_dict['key']), string_clean(att_dict['value']['value'])]\n",
    "\n",
    "                for qk, qvs in att_dict['qualifiers'].items():                \n",
    "                    # Third: add qk - qv pairs, for qv that have more than one instance, seperate to single qk - qv pairs\n",
    "                    new_qvs = []\n",
    "                    for qv in qvs:\n",
    "                        if qv['type'] == 'string':\n",
    "                            new_qvs.append(string_clean(qv['value']))\n",
    "                            \n",
    "                    if len(new_qvs) != 0:\n",
    "                        for qv in new_qvs:\n",
    "                            statement += [string_clean(qk), qv]\n",
    "            \n",
    "                # Fourth: Make sure the statement is qualifier \n",
    "                if len(statement) > 3:\n",
    "                    qualifier.add(tuple(statement))\n",
    "\n",
    "    # qualifier = sorted(qualifier)\n",
    "\n",
    "    if output:\n",
    "        str_q = [\",\".join(q)+'\\n' for q in qualifier]\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.writelines(str_q)\n",
    "    \n",
    "    return qualifier\n",
    "\n",
    "def get_attributes_clean_fullname(kb_json, output=False, file_name='kb_a_clean_fullname.txt'):\n",
    "    qualifier = set()\n",
    "    kb = json.load(open(kb_json))\n",
    "    for i in kb['entities']:\n",
    "        fullname = kb['entities'][i]['name']\n",
    "\n",
    "        # For attribute\n",
    "        for att_dict in kb['entities'][i]['attributes']:\n",
    "            # First: if it is literal, ignore it\n",
    "            if att_dict['value']['type'] != 'string':\n",
    "                continue\n",
    "            else:\n",
    "                # Second: add attributes\n",
    "                statement = list()\n",
    "                statement += [string_clean(fullname), string_clean(att_dict['key']), string_clean(att_dict['value']['value'])]\n",
    "\n",
    "                for qk, qvs in att_dict['qualifiers'].items():                \n",
    "                    # Third: add qk - qv pairs, for qv that have more than one instance, seperate to single qk - qv pairs\n",
    "                    new_qvs = []\n",
    "                    for qv in qvs:\n",
    "                        if qv['type'] == 'string':\n",
    "                            new_qvs.append(string_clean(qv['value']))\n",
    "                            \n",
    "                    if len(new_qvs) != 0:\n",
    "                        for qv in new_qvs:\n",
    "                            statement += [string_clean(qk), qv]\n",
    "            \n",
    "                # Fourth: Add statement\n",
    "                qualifier.add(tuple(statement))\n",
    "\n",
    "    qualifier = sorted(qualifier)\n",
    "\n",
    "    if output:\n",
    "        str_q = [\",\".join(q)+'\\n' for q in qualifier]\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.writelines(str_q)\n",
    "    \n",
    "    return qualifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_clean_fullname(kb_json, output=False, file_name='kb_all_clean_fullname.txt'):\n",
    "    qualifier = set()\n",
    "    kb = json.load(open(kb_json))\n",
    "    for i in kb['entities']:\n",
    "        fullname = kb['entities'][i]['name']\n",
    "\n",
    "        # For instance of\n",
    "        for concept_id in kb['entities'][i]['instanceOf']:\n",
    "            statement = [string_clean(fullname), 'instance of', string_clean(find_name(kb, concept_id))]\n",
    "            qualifier.add(tuple(statement))\n",
    "\n",
    "        # For attribute\n",
    "        for att_dict in kb['entities'][i]['attributes']:\n",
    "            # First: if it is literal, ignore it\n",
    "            if att_dict['value']['type'] != 'string':\n",
    "                continue\n",
    "            else:\n",
    "                # Second: add attributes\n",
    "                statement = list()\n",
    "                statement += [string_clean(fullname), string_clean(att_dict['key']), string_clean(att_dict['value']['value'])]\n",
    "\n",
    "                for qk, qvs in att_dict['qualifiers'].items():                \n",
    "                    # Third: add qk - qv pairs, for qv that have more than one instance, seperate to single qk - qv pairs\n",
    "                    new_qvs = []\n",
    "                    for qv in qvs:\n",
    "                        if qv['type'] == 'string':\n",
    "                            new_qvs.append(string_clean(qv['value']))\n",
    "                            \n",
    "                    if len(new_qvs) != 0:\n",
    "                        for qv in new_qvs:\n",
    "                            statement += [string_clean(qk), qv]\n",
    "            \n",
    "                # Fourth: Add statement\n",
    "                qualifier.add(tuple(statement))\n",
    "\n",
    "        # For relation\n",
    "        for rel_dict in kb['entities'][i]['relations']:\n",
    "            # First: add fact key, also called triple pairs\n",
    "            statement = list()\n",
    "            if rel_dict['direction'] == 'forward':\n",
    "                statement += [string_clean(fullname), string_clean(rel_dict['predicate']), string_clean(find_name(kb, rel_dict['object']))]\n",
    "            elif  rel_dict['direction'] == 'backward':\n",
    "                statement += [string_clean(find_name(kb, rel_dict['object'])), string_clean(rel_dict['predicate']), string_clean(fullname)]\n",
    "\n",
    "            for qk, qvs in rel_dict['qualifiers'].items():                \n",
    "                # Second add qk - qv pairs, for qv that have more than one instance, seperate to single qk - qv pairs\n",
    "                new_qvs = []\n",
    "                for qv in qvs:\n",
    "                    if qv['type'] == 'string':\n",
    "                        new_qvs.append(string_clean(qv['value']))\n",
    "                        \n",
    "                if len(new_qvs) != 0:\n",
    "                    for qv in new_qvs:\n",
    "                        statement += [string_clean(qk), qv]\n",
    "        \n",
    "            # Third: Add statement \n",
    "            qualifier.add(tuple(statement))\n",
    "\n",
    "    qualifier = sorted(qualifier)\n",
    "\n",
    "    if output:\n",
    "        str_q = [\",\".join(q)+'\\n' for q in qualifier]\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.writelines(str_q)\n",
    "    \n",
    "    return qualifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = get_all_clean_fullname(kb_json, output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q(kb_json, output=False, file_name='test.txt'):\n",
    "    qualifier = set()\n",
    "    kb = json.load(open(kb_json))\n",
    "    for i in kb['entities']:\n",
    "        for rel_dict in kb['entities'][i]['relations']:\n",
    "            # First: add fact key, also called triple pairs\n",
    "            statement = list()\n",
    "            if rel_dict['direction'] == 'forward':\n",
    "                statement += [i, string_clean(rel_dict['predicate']), rel_dict['object']]\n",
    "            elif  rel_dict['direction'] == 'backward':\n",
    "                statement += [rel_dict['object'], string_clean(rel_dict['predicate']), i]\n",
    "\n",
    "            for qk, qvs in rel_dict['qualifiers'].items():                \n",
    "                # Second add qk - qv pairs, for qv that have more than one instance, seperate to single qk - qv pairs\n",
    "                new_qvs = []\n",
    "                for qv in qvs:\n",
    "                    if qv['type'] == 'string':\n",
    "                        new_qvs.append(string_clean(qv['value']))\n",
    "                        \n",
    "                if len(new_qvs) != 0:\n",
    "                    for qv in new_qvs:\n",
    "                        statement += [string_clean(qk), qv]\n",
    "        \n",
    "            # Third: Make sure the statement is qualifier \n",
    "            if len(statement) > 3:\n",
    "                qualifier.add(tuple(statement))\n",
    "    \n",
    "    qualifier = list(qualifier)\n",
    "    new_qualifier = []\n",
    "\n",
    "    for statement in qualifier:\n",
    "        new_statement = list(statement)\n",
    "        new_statement[0] = string_clean(find_name(kb, statement[0]))\n",
    "        new_statement[2] = string_clean(find_name(kb, statement[2]))\n",
    "        new_qualifier.append(tuple(new_statement))\n",
    "\n",
    "    new_qualifier = sorted(new_qualifier)\n",
    "\n",
    "    if output:\n",
    "        str_q = [\",\".join(q)+'\\n' for q in new_qualifier]\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.writelines(str_q)\n",
    "\n",
    "    return new_qualifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(s: set, split: list=[0.85, 0.15]):\n",
    "    str_l = [\",\".join(q)+'\\n' for q in s]\n",
    "    str_l = np.array(str_l)\n",
    "    length = len(str_l)\n",
    "    permutation = np.random.permutation(length).reshape(-1)\n",
    "    trn_length = np.round(length * split[0]).astype(int)\n",
    "    # vld_length = np.round(length * split[1])\n",
    "    tst_length = np.round(length * split[1]).astype(int)\n",
    "    # assert (trn_length + vld_length + tst_length) == length\n",
    "    assert (trn_length + tst_length) == length\n",
    "    trn = str_l[permutation[0:trn_length]]\n",
    "    # vld = str_l[permutation[trn_length:trn_length+vld_length]]\n",
    "    # tst = str_l[permutation[trn_length+vld_length:length]]\n",
    "    tst = str_l[permutation[trn_length:length]]\n",
    "\n",
    "    with open(\"train.txt\", 'w')as f:\n",
    "        f.writelines(trn)\n",
    "    with open(\"test.txt\", 'w')  as f:\n",
    "        f.writelines(tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94376, 11797)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = json.load(open(train_json))\n",
    "test = json.load(open(val_json))\n",
    "length_train = len(train)\n",
    "length_test = len(test)\n",
    "length_train, length_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = json.load(open(train_json))\n",
    "# length = len(train)\n",
    "for i in range(100):\n",
    "    q = train[i]\n",
    "    # print(q['program'])\n",
    "    # print(q['sparql'])\n",
    "    # print(q['answer'])\n",
    "    # print(q['choices'])\n",
    "    # print(q['question'])\n",
    "    # print()\n",
    "    if 'COUNT' in q['sparql']:\n",
    "        print(q['sparql'])\n",
    "        print(q['choices'])\n",
    "        print(q['answer'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The first type of program: QueryName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QueryName = list()\n",
    "for statement in train:\n",
    "    if statement['program'][0]['function'] == 'Find' and statement['program'][-1]['function'] == 'What':\n",
    "        QueryName.append(statement)\n",
    "len(QueryName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Program to query graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def program_to_graph(program):\n",
    "\n",
    "    entities = dict()\n",
    "    triple = list()\n",
    "\n",
    "    for idx, block in enumerate(program):\n",
    "        function = block['function']\n",
    "        dependencies = block['dependencies']\n",
    "        inputs = block['inputs']\n",
    "\n",
    "        if function == 'FindAll':\n",
    "            '''\n",
    "            Find all entities in the kb. This is hard to dispose.\n",
    "            '''\n",
    "            pass\n",
    "        elif function == 'Find':\n",
    "            '''\n",
    "            Find all entities with the name.\n",
    "            '''\n",
    "            entities[idx] = inputs[0]\n",
    "            \n",
    "        elif function == 'FilterConcept':\n",
    "            pass\n",
    "            entities[idx] = inputs[0]\n",
    "\n",
    "        elif function in ['FilterStr', 'FilterNum', 'FilterYear', 'FilterDate']:\n",
    "            pass\n",
    "        elif function in ['QFilterStr', 'QFilterNum', 'QFilterYear', 'QFilterDate']:\n",
    "            pass\n",
    "        elif function in 'Relate':\n",
    "            pass\n",
    "        elif function in ['And', 'Or']:\n",
    "            pass\n",
    "        elif function == 'What':\n",
    "            pass\n",
    "        elif function == 'Count':\n",
    "            pass\n",
    "        elif function in ['QueryAttr', 'QueryAttrUnderCondition']:\n",
    "            pass\n",
    "        elif function == 'QueryRelation':\n",
    "            pass\n",
    "        elif function in ['SelectBetween', 'SelectAmong']:\n",
    "            pass\n",
    "        elif function in  ['VerifyStr', 'VerifyNum', 'VerifyYear', 'VerifyDate']:\n",
    "            pass\n",
    "        elif function in ['QueryAttrQualifier', 'QueryRelationQualifier']:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPARQL to query guery graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import shlex\n",
    "import copy\n",
    "\n",
    "def string_clean(s: str) -> str:\n",
    "    s = s.replace(',', ' and ')\n",
    "    s = ' '.join(s.split())\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparql_to_graph(sparql):\n",
    "    PRED_INSTANCE = 'pred:instance_of'\n",
    "    PRED_NAME = 'pred:name'\n",
    "\n",
    "    PRED_VALUE = 'pred:value'       # link packed value node to its literal value\n",
    "    PRED_UNIT = 'pred:unit'         # link packed value node to its unit\n",
    "\n",
    "    PRED_YEAR = 'pred:year'         # link packed value node to its year value, which is an integer\n",
    "    PRED_DATE = 'pred:date'         # link packed value node to its date value, which is a date\n",
    "\n",
    "    PRED_FACT_H = 'pred:fact_h'     # link qualifier node to its head\n",
    "    PRED_FACT_R = 'pred:fact_r'\n",
    "    PRED_FACT_T = 'pred:fact_t'\n",
    "\n",
    "    SPECIAL_PREDICATES = (PRED_INSTANCE, PRED_NAME, PRED_VALUE, PRED_UNIT, PRED_YEAR, PRED_DATE, PRED_FACT_H, PRED_FACT_R, PRED_FACT_T)\n",
    "\n",
    "    target = None\n",
    "\n",
    "    \"\"\"\n",
    "    Some sparql have UNION inside. Ingore them at this stage.\n",
    "    \"\"\"\n",
    "\n",
    "    if sparql.startswith('SELECT DISTINCT ?e'):\n",
    "        parse_type = 'entity'\n",
    "        target = '?e'\n",
    "    elif sparql.startswith('SELECT ?e'):\n",
    "        parse_type = 'sort'\n",
    "        target = 'first ?e'\n",
    "    elif sparql.startswith('SELECT (COUNT(DISTINCT ?e)'):\n",
    "        parse_type = 'count'\n",
    "        target = 'count ?e'\n",
    "    elif sparql.startswith('SELECT DISTINCT ?p '):\n",
    "        parse_type = 'pred'\n",
    "        target = '?p'\n",
    "    elif sparql.startswith('ASK'):\n",
    "        parse_type = 'bool'\n",
    "        target = 'bool'\n",
    "    else:\n",
    "        parse_type = 'attr'\n",
    "        tokens = sparql.split()\n",
    "        target = tokens[2]\n",
    "        \n",
    "        \"\"\"\n",
    "        Should consider attributes selection here, but it is complex at first glance. Ignore it first and \n",
    "        I will implemented it later\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    case = 0\n",
    "    triples = None\n",
    "\n",
    "    '''\n",
    "    Check if sort\n",
    "    '''\n",
    "    sort_identity = sparql.split('{', maxsplit=1)[1].rsplit('}', maxsplit=1)[1]\n",
    "    \n",
    "    '''\n",
    "    0 - Normal case\n",
    "    1 - UNION exist\n",
    "    2 - \"\\'\" exist\n",
    "    3 -  BOTH 1 and 2 happens\n",
    "    4 - 2 happens and \"\\'\" in pred and will cause the shlex throw error\n",
    "    '''\n",
    "    if 'UNION' in sparql:\n",
    "        case = 1\n",
    "        triples = sparql.split('{', maxsplit=1)[1].rsplit('}', maxsplit=1)[0]\n",
    "        match = re.fullmatch(r'''(.*?){(.*?)} UNION {(.*?)}(.*)''', triples)\n",
    "        four_triples = match.groups()\n",
    "        '''\n",
    "        Now the four_triples contain four triples and [2:3] are union based\n",
    "        '''\n",
    "        triples = []\n",
    "        for idx, group in enumerate(four_triples):\n",
    "            _gs = re.split(r'''\\.(?=(?:[^\"]|\"[^\"]*\")*$)''', group)\n",
    "            _gs = [_g.strip() for _g in _gs]\n",
    "            if idx == 0:\n",
    "                _gs.append('SEPARATER{')\n",
    "            elif idx == 1:\n",
    "                _gs.append('}SEPARATER{')\n",
    "            elif idx == 2:\n",
    "                _gs.append('}SEPARATER')\n",
    "            triples += _gs\n",
    "\n",
    "    if '\\'' in sparql:\n",
    "        case = 2 if case == 0 else 3\n",
    "        if case == 2:\n",
    "            triples = sparql.split('{')[1].split('}')[0]\n",
    "            triples = re.split(r'''\\.(?=(?:[^\"]|\"[^\"]*\")*$)''', triples)\n",
    "            triples = [triple.strip() for triple in triples]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if case == 0: # Normal case\n",
    "        triples = sparql.split('{')[1].split('}')[0]\n",
    "        triples = re.split(r'''\\.(?=(?:[^\"]|\"[^\"]*\")*$)''', triples)\n",
    "        triples = [triple.strip() for triple in triples]\n",
    "    \n",
    "    '''\n",
    "    Match the space: if there are even number of \" or ' after the space, use it as the delimilator\n",
    "    The re is hard to match escape quote\n",
    "    '''\n",
    "    seperated_triples = []\n",
    "\n",
    "    if case == 2 or case == 3:\n",
    "        '''\n",
    "        There is a case that r\\' in pred and make the whole string quote in double quotes and cannot be recognized by shlex\n",
    "        A question that index is 3060~3070 in train is a case\n",
    "        '''\n",
    "        for triple in triples:\n",
    "            try:\n",
    "                new_triple = shlex.split(triple)\n",
    "            except:\n",
    "                case = 4\n",
    "                return case, target, parse_type, None\n",
    "            if not (len(new_triple) == 0 or new_triple == ''):\n",
    "                seperated_triples.append(new_triple)\n",
    "    if case == 0 or case == 1:\n",
    "        for triple in triples:\n",
    "            new_triple = shlex.split(triple)\n",
    "            if not (len(new_triple) == 0 or new_triple == ''):\n",
    "                seperated_triples.append(new_triple)\n",
    "    \n",
    "    '''\n",
    "    Now make all seperated triple to the format we want\n",
    "    '''\n",
    "    disposed_triples = []\n",
    "    for triple in seperated_triples:\n",
    "        if len(triple) == 3:\n",
    "            r = triple[1]\n",
    "            if r.startswith('<'):\n",
    "                if PRED_INSTANCE in r:\n",
    "                    # Ignore pred\n",
    "                    relation = r[6:-1].replace('_', ' ')\n",
    "                    new_triple = (string_clean(triple[0]), string_clean(relation), string_clean(triple[2]))\n",
    "                    disposed_triples.append(new_triple)\n",
    "                # elif PRED_NAME in r:\n",
    "                #     pass\n",
    "                # elif PRED_VALUE in r:\n",
    "                #     pass\n",
    "                # elif PRED_UNIT in r:\n",
    "                #     pass\n",
    "                # elif PRED_YEAR in r:\n",
    "                #     pass\n",
    "                # elif PRED_DATE in r:\n",
    "                #     pass\n",
    "                else:\n",
    "                    # A normal predicate/relation \n",
    "                    relation = r[1:-1].replace('_', ' ')\n",
    "                    new_triple = (string_clean(triple[0]), string_clean(relation), string_clean(triple[2]))\n",
    "                    disposed_triples.append(new_triple)\n",
    "            else:\n",
    "                new_triple = (string_clean(triple[0]), string_clean(triple[1]), string_clean(triple[2]))\n",
    "                disposed_triples.append(new_triple)\n",
    "\n",
    "        elif len(triple) != 3:\n",
    "            if triple[0] == '[':\n",
    "                pred = triple[10]\n",
    "                if pred.startswith('<'):\n",
    "                    pred = pred[1:-1].replace('_', ' ')\n",
    "                attr = triple[11]\n",
    "                fact_h = triple[2]\n",
    "                fact_r = triple[5]\n",
    "                if fact_r.startswith('<'):\n",
    "                    fact_r = fact_r[1:-1].replace('_', ' ')\n",
    "                fact_t = triple[8]\n",
    "                qualifier_nodes = [string_clean(fact_h), string_clean(fact_r), string_clean(fact_t)]\n",
    "                new_triple = (*qualifier_nodes, string_clean(pred), string_clean(attr))\n",
    "                disposed_triples.append(new_triple)\n",
    "            elif triple[0] == 'FILTER':\n",
    "                new_triple = \" \".join(triple)\n",
    "                disposed_triples.append(tuple([new_triple]))\n",
    "            elif 'SEPARATER' in triple[0]:\n",
    "                disposed_triples.append(tuple(triple))\n",
    "\n",
    "    '''\n",
    "    Add order finally\n",
    "    '''\n",
    "    if 'ORDER' in sort_identity:\n",
    "        disposed_triples.append(tuple([string_clean(sort_identity)]))\n",
    "    '''\n",
    "    Post process\n",
    "    '''\n",
    "    if parse_type == 'name':\n",
    "        pass\n",
    "    elif parse_type == 'count':\n",
    "        pass\n",
    "    elif parse_type == 'bool':\n",
    "        pass\n",
    "    elif parse_type == 'pred':\n",
    "        pass\n",
    "    elif parse_type == 'attr': \n",
    "        pass\n",
    "\n",
    "    return case, parse_type, target, disposed_triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some previous test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sparql_triple.txt', 'w') as f:\n",
    "    for i in range(length_train):\n",
    "        test = train[i]['sparql']\n",
    "        c, t, d = sparql_to_graph(test)\n",
    "        f.write(f'target:{t}\\n')\n",
    "        if d is not None:\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\n')\n",
    "            f.write('\\n')\n",
    "        else:\n",
    "            f.write('None')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statement_simplification(case, target, statement):\n",
    "    if case == 4:\n",
    "        return None\n",
    "\n",
    "    statement_refine = []\n",
    "    for triple in statement:\n",
    "        if len(triple) == 3:\n",
    "            if triple[1].startswith('pred:') and triple[1] != 'pred:instance of' and triple[1] != 'pred:name':\n",
    "                pass\n",
    "            elif triple[1] == 'pred:instance of':\n",
    "                statement_refine.append((triple[0], 'instance of', triple[2]))\n",
    "            elif triple[1] == 'pred:name':\n",
    "                statement_refine.append((triple[0], 'NAME'))\n",
    "            else:\n",
    "                if triple[1] != '?p':\n",
    "                    if 'e' in triple[2]:\n",
    "                        statement_refine.append((triple[0], 'relational', triple[2]))\n",
    "                    else:\n",
    "                        statement_refine.append((triple[0], 'literal', triple[2]))\n",
    "                else:\n",
    "                    statement_refine.append(triple)\n",
    "        elif len(triple) != 3:\n",
    "            if 'FILTER' in triple[0]:\n",
    "                statement_refine.append(tuple(['OPERATION']))\n",
    "            elif 'SEPARATER' in triple[0]:\n",
    "                if triple[0] == 'SEPARATER{':\n",
    "                    statement_refine.append(tuple(['[']))\n",
    "                elif triple[0] == '}SEPARATER{':\n",
    "                    statement_refine.append(tuple(['UNION']))\n",
    "                else:\n",
    "                    statement_refine.append(tuple([']']))\n",
    "            elif len(triple) == 5: # Must be qualifier\n",
    "                if 'e' in triple[2]:\n",
    "                    statement_refine.append((triple[0], 'relational', triple[2], 'qualifier', triple[4]))\n",
    "                else:\n",
    "                    statement_refine.append((triple[0], 'literal', triple[2], 'qualifier', triple[4]))\n",
    "    return statement_refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_t = []\n",
    "with open('test_origin.txt', 'w') as f, open('test_simply.txt', 'w') as fs:\n",
    "    for i in range(length):\n",
    "        test = train[i]['sparql']\n",
    "        c, t, d = sparql_to_graph(test)\n",
    "        test_t.append((t, d))\n",
    "        sf = statement_simplification(c, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            f.write(train[i]['question'] + '\\n')\n",
    "            f.write(f'ID: {i}, target:{t}\\n---\\n')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\n')\n",
    "            f.write('---\\n')\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "        if sf is not None:\n",
    "            fs.write(train[i]['question'] + '\\n')\n",
    "            fs.write(f'ID: {i}, target:{t}\\n---\\n')\n",
    "            for a in sf:\n",
    "                str_a = \",\".join(a)\n",
    "                fs.writelines(str_a)\n",
    "                fs.write('\\n')\n",
    "            fs.write('---\\n')\n",
    "            fs.write('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 264\n",
    "print(train[x]['sparql'])\n",
    "print(train[x]['program'])\n",
    "c, t, d = sparql_to_graph(train[x]['sparql'])\n",
    "c, t, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter out relational query and simplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_simplifier_rough_no_literal(case, parse_type, target, query_graph):\n",
    "\n",
    "    PRED_VALUE = 'pred:value'       # link packed value node to its literal value\n",
    "    PRED_UNIT = 'pred:unit'         # link packed value node to its unit\n",
    "\n",
    "    PRED_YEAR = 'pred:year'         # link packed value node to its year value, which is an integer\n",
    "    PRED_DATE = 'pred:date'  \n",
    "\n",
    "    if (case == 1) or (case == 3) or (case == 4):\n",
    "        return\n",
    "    for statement in query_graph:\n",
    "        if len(statement) > 1:\n",
    "            if (PRED_UNIT in statement[1]) or (PRED_YEAR in statement[1]) or (PRED_DATE in statement[1]):\n",
    "                return\n",
    "\n",
    "    substitution_name_dict = dict()\n",
    "    substitution_value_dict = dict()\n",
    "    for statement in query_graph:\n",
    "        if len(statement) == 3: # Normal case, no qualifier, no filter\n",
    "            if statement[1] == 'pred:name':\n",
    "                substitution_name_dict[statement[0]] = statement[2]\n",
    "            if statement[1] == 'pred:value':\n",
    "                substitution_value_dict[statement[0]] = statement[2]\n",
    "\n",
    "    output_graph = []\n",
    "    name_keys = list(substitution_name_dict.keys())\n",
    "    value_keys = list(substitution_value_dict.keys())\n",
    "    for statement in query_graph:\n",
    "        if len(statement) == 3 or len(statement) == 5: # Normal case, no qualifier, no filter\n",
    "            if statement[1] == 'pred:name' or statement[1] == 'pred:value':\n",
    "                pass\n",
    "            else:\n",
    "                ### Refine the statement\n",
    "                if  len(statement) == 3:\n",
    "                    new_statement_0 = statement[0]\n",
    "                    new_statement_2 = statement[2]\n",
    "                    if statement[0] in name_keys:\n",
    "                        new_statement_0 = substitution_name_dict[statement[0]]\n",
    "                    if statement[2] in name_keys:\n",
    "                        new_statement_2 = substitution_name_dict[statement[2]]\n",
    "\n",
    "                    if statement[0] in value_keys:\n",
    "                        new_statement_0 = substitution_value_dict[statement[0]]\n",
    "                    if statement[2] in value_keys:\n",
    "                        new_statement_2 = substitution_value_dict[statement[2]]\n",
    "                    \n",
    "                    output_graph.append((new_statement_0, statement[1], new_statement_2))\n",
    "                elif len(statement) == 5:\n",
    "                    new_statement_0 = statement[0]\n",
    "                    new_statement_2 = statement[2]\n",
    "                    new_statement_4 = statement[4]\n",
    "                    if statement[0] in name_keys:\n",
    "                        new_statement_0 = substitution_name_dict[statement[0]]\n",
    "                    if statement[2] in name_keys:\n",
    "                        new_statement_2 = substitution_name_dict[statement[2]]\n",
    "                    if statement[4] in name_keys:\n",
    "                        new_statement_4 = substitution_name_dict[statement[4]]\n",
    "                    \n",
    "                    if statement[0] in value_keys:\n",
    "                        new_statement_0 = substitution_value_dict[statement[0]]\n",
    "                    if statement[2] in value_keys:\n",
    "                        new_statement_2 = substitution_value_dict[statement[2]]\n",
    "                    if statement[4] in value_keys:\n",
    "                        new_statement_4 = substitution_value_dict[statement[4]]\n",
    "                    \n",
    "                    output_graph.append((new_statement_0, statement[1], new_statement_2, statement[3], new_statement_4))\n",
    "        else:\n",
    "            # print(\"Special graph\")\n",
    "            output_graph.append(statement)\n",
    "    \n",
    "    # Check redundant qualifier and triple\n",
    "    statement_need_remove = []\n",
    "    for statement in output_graph:\n",
    "        if len(statement) == 5:\n",
    "            for st in output_graph:\n",
    "                if len(st) == 3 and (st[0] == statement[0]) and (st[1] == statement[1]) and (st[0] == statement[0]):\n",
    "                    statement_need_remove.append(st)\n",
    "    for st in statement_need_remove:\n",
    "        output_graph.remove(st)\n",
    "\n",
    "    return output_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52547"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relational = 0\n",
    "with open('test_no_literal_modified.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_train):\n",
    "        test = train[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(test)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            f.write(train[i]['question'] + '\\n')\n",
    "            f.write(f'ID: {i}, target:{t}\\n---\\n')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\n')\n",
    "            f.write('---\\n') \n",
    "            f.write(train[i]['answer'] + '\\n')\n",
    "            f.write('\\n')       \n",
    "            relational += 1\n",
    "relational "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further filter out as multihop, qualifier or verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The graph to this step: have processed so that no \"UNION\", no \"FILTER\", no \"<pred:...>\"\n",
    "'''\n",
    "\n",
    "def retrieve_multihop(case, parse_type, target, query_graph):\n",
    "    entities = set()\n",
    "    if parse_type == 'count':\n",
    "        return None\n",
    "    if query_graph is None:\n",
    "        return None\n",
    "\n",
    "    for statement in query_graph:\n",
    "        if '?e' in statement[0]:\n",
    "            entities.add(statement[0])\n",
    "        if '?e' in statement[2]:\n",
    "            entities.add(statement[2])\n",
    "    \n",
    "    if len(entities) > 1:\n",
    "        return query_graph\n",
    "    return None\n",
    "\n",
    "def retrieve_qualifier_qpv(case, parse_type, target, query_graph):\n",
    "    if parse_type == 'attr' and target == '?qpv':\n",
    "        return query_graph\n",
    "    return None\n",
    "\n",
    "def retrieve_qualifier_other(case, parse_type, target, query_graph):\n",
    "    if parse_type == 'attr' and target == '?qpv':\n",
    "        return None\n",
    "    if query_graph is None:\n",
    "        return None\n",
    "    for statement in query_graph:\n",
    "        if len(statement) == 5:\n",
    "            return query_graph\n",
    "    return None\n",
    "\n",
    "def retrieve_verify(case, parse_type, target, query_graph):\n",
    "    if parse_type == 'bool' and target == 'bool':\n",
    "        return query_graph\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_entity(case, parse_type, target, query_graph):\n",
    "\n",
    "    if target == '?e':\n",
    "        return query_graph\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qpv_no_literal(choice_0):\n",
    "    try:\n",
    "        c = float(choice_0)\n",
    "    except:\n",
    "        c = re.fullmatch('''\\d{4}-\\d{2}-\\d{2}''', choice_0)\n",
    "        if c is None:\n",
    "            return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_id(target, query_graph, id):\n",
    "    \n",
    "    if ('?e' in target) or ('?pv' in target) or ('?qpv' in target) or ('?p' in target):\n",
    "        id_target = target + '_' + str(id)\n",
    "    else:\n",
    "        id_target = target\n",
    "\n",
    "    id_graph = []\n",
    "    for statement in query_graph:\n",
    "        if len(statement) == 3:\n",
    "            s, r, o = statement\n",
    "            if ('?e' in s) or ('?pv' in s):\n",
    "                s = s  + '_' + str(id)\n",
    "            if ('?e' in o) or ('?pv' in o):\n",
    "                o = o  + '_' + str(id)\n",
    "            if '?p' in r:\n",
    "                r = r + '_' + str(id)\n",
    "            id_graph.append((s, r, o))\n",
    "            \n",
    "        if len(statement) == 5:\n",
    "            s, r, o, k, v = statement\n",
    "            if ('?e' in s) or ('?pv' in s):\n",
    "                s = s  + '_' + str(id)\n",
    "            if ('?e' in o) or ('?pv' in o):\n",
    "                o = o  + '_' + str(id)\n",
    "            if '?p' in r:\n",
    "                r = r + '_' + str(id)\n",
    "            if '?qpv' in v:\n",
    "                v = v + '_' + str(id)\n",
    "            id_graph.append((s, r, o, k, v))\n",
    "            \n",
    "    return id_target, id_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate training/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1875"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./multihop/train.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_train):\n",
    "        qry = train[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_multihop(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            choices_list = [string_clean(s) for s in train[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(train[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./multihop/test.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_test):\n",
    "        qry = test[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_multihop(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            choices_list = [string_clean(s) for s in test[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(test[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10555"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./qualifier/train.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_train):\n",
    "        qry = train[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_qualifier_qpv(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            choices_list = [string_clean(s) for s in train[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(train[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1316"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./qualifier/test.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_test):\n",
    "        qry = test[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_qualifier_qpv(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            choices_list = [string_clean(s) for s in test[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(test[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1533"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./qualifier/train_2.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_train):\n",
    "        qry = train[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_qualifier_other(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            choices_list = [string_clean(s) for s in train[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(train[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./qualifier/test_2.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_test):\n",
    "        qry = test[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_qualifier_other(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            choices_list = [string_clean(s) for s in test[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(test[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4961"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./verify/train.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_train):\n",
    "        qry = train[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_verify(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(train[i]['answer'])\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./verify/test.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_test):\n",
    "        qry = test[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_verify(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(test[i]['answer'])\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate ID training/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1875"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./multihop/train_id.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_train):\n",
    "        qry = train[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_multihop(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            t, d = add_id(t, d, i)\n",
    "            choices_list = [string_clean(s) for s in train[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(train[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./multihop/test_id.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_test):\n",
    "        qry = test[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_multihop(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            t, d = add_id(t, d, i)\n",
    "            choices_list = [string_clean(s) for s in test[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(test[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10555"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./qualifier/train_id.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_train):\n",
    "        qry = train[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_qualifier_qpv(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            t, d = add_id(t, d, i)\n",
    "            choices_list = [string_clean(s) for s in train[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(train[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1316"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./qualifier/test_id.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_test):\n",
    "        qry = test[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_qualifier_qpv(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            t, d = add_id(t, d, i)\n",
    "            choices_list = [string_clean(s) for s in test[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(test[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4961"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./verify/train_id.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_train):\n",
    "        qry = train[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_verify(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            for a in d:\n",
    "                t, d = add_id(t, d, i)\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(train[i]['answer'])\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./verify/test_id.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_test):\n",
    "        qry = test[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_verify(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            t, d = add_id(t, d, i)\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(test[i]['answer'])\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate dataset with pure type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8525"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./entity/train.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_train):\n",
    "        qry = train[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_entity(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            choices_list = [string_clean(s) for s in train[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(train[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1142"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./entity/test.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_test):\n",
    "        qry = test[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_entity(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            choices_list = [string_clean(s) for s in test[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(test[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8525"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./entity/train_id.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_train):\n",
    "        qry = train[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_entity(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            t, d = add_id(t, d, i)\n",
    "            choices_list = [string_clean(s) for s in train[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(train[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1142"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./entity/test_id.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_test):\n",
    "        qry = test[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_entity(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            t, d = add_id(t, d, i)\n",
    "            choices_list = [string_clean(s) for s in test[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(test[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5700"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./qualifier/train_id_clean.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_train):\n",
    "        qry = train[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_qualifier_qpv(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            t, d = add_id(t, d, i)\n",
    "            choices_list = [string_clean(s) for s in train[i]['choices']]\n",
    "\n",
    "            flag = qpv_no_literal(choices_list[0])\n",
    "            if not flag: continue\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(train[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('./qualifier/test_id_clean.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(length_test):\n",
    "        qry = test[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = graph_simplifier_rough_no_literal(c, p, t, d)\n",
    "        d = retrieve_qualifier_qpv(c, p, t, d)\n",
    "\n",
    "        if d is not None:\n",
    "            t, d = add_id(t, d, i)\n",
    "            choices_list = [string_clean(s) for s in test[i]['choices']]\n",
    "            flag = qpv_no_literal(choices_list[0])\n",
    "            if not flag: continue\n",
    "            \n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(test[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find some cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_target_entity_without_filter(case, parse_type, target, query_graph):\n",
    "    if case != 4 and target == '?e' and parse_type == 'entity':\n",
    "        return query_graph\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entity_qry.txt', 'w') as f:\n",
    "    for i in range(length_train):\n",
    "        qry = train[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = find_all_target_entity(c, p, t, d)\n",
    "        \n",
    "        if d is not None:\n",
    "            choices_list = [string_clean(s) for s in train[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(train[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_compare_entity(case, parse_type, target, query_graph):\n",
    "    if case != 4 and target == '?e' and parse_type == 'sort':\n",
    "        return query_graph\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open('entity_compare.txt', 'w') as f:\n",
    "    for i in range(length_train):\n",
    "        qry = train[i]['sparql']\n",
    "        c, p, t, d = sparql_to_graph(qry)\n",
    "        d = find_all_compare_entity(c, p, t, d)\n",
    "        \n",
    "        if d is not None:\n",
    "            t, d = add_id(t, d, i)\n",
    "            choices_list = [string_clean(s) for s in train[i]['choices']]\n",
    "            choices = \",\".join(choices_list)\n",
    "\n",
    "            f.write(t + '\\t')\n",
    "            for a in d:\n",
    "                str_a = \",\".join(a)\n",
    "                f.writelines(str_a)\n",
    "                f.write('\\t')\n",
    "            f.write(choices + '\\t')\n",
    "            f.write(string_clean(train[i]['answer']))\n",
    "            f.write('\\n')       \n",
    "            count += 1\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "485a81571bad82d07f9163ec6f245eb6726e96d35f7b1b2a82e1d043f346be54"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
