{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_json = '../dataset/kb.json'\n",
    "\n",
    "train_json = '../dataset/train.json'\n",
    "val_json = '../dataset/val.json'\n",
    "test_json = '../dataset/test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_clean(s: str) -> str:\n",
    "    s = s.replace(',', ' and ')\n",
    "    s = ' '.join(s.split())\n",
    "    return s\n",
    "\n",
    "def find_name(kb, id):\n",
    "    try:\n",
    "        return kb['entities'][id]['name']\n",
    "    except:\n",
    "        try:\n",
    "            return kb['concepts'][id]['name']\n",
    "        except:\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Different version that have different sort\n",
    "# def get_qualifier_relational_clean_fullname_combine(kb_json, output=False, file_name='kb_q_r_clean_fullname_combine.txt'):\n",
    "#     qualifier = dict()\n",
    "#     kb = json.load(open(kb_json))\n",
    "#     for i in kb['entities']:\n",
    "#         fullname = kb['entities'][i]['name']\n",
    "#         for rel_dict in kb['entities'][i]['relations']:\n",
    "#             # First: add fact key, also called triple pairs\n",
    "#             statement = None\n",
    "#             if rel_dict['direction'] == 'forward':\n",
    "#                 statement = (string_clean(fullname), string_clean(rel_dict['predicate']), string_clean(find_name(kb, rel_dict['object'])))\n",
    "#             elif  rel_dict['direction'] == 'backward':\n",
    "#                 statement = (string_clean(find_name(kb, rel_dict['object'])), string_clean(rel_dict['predicate']), string_clean(fullname))\n",
    "            \n",
    "#             if not statement in qualifier: \n",
    "#                 qualifier[statement] = set()\n",
    "\n",
    "#             for qk, qvs in rel_dict['qualifiers'].items():                \n",
    "#                 # Second add qk - qv pairs, for qv that have more than one instance, seperate to single qk - qv pairs\n",
    "#                 new_qvs = []\n",
    "#                 for qv in qvs:\n",
    "#                     if qv['type'] == 'string':\n",
    "#                         new_qvs.append(string_clean(qv['value']))\n",
    "                        \n",
    "#                 if len(new_qvs) != 0:\n",
    "#                     for qv in new_qvs:\n",
    "#                         # Add as pairs so that can do duplication check\n",
    "#                         qualifier[statement].add(tuple([string_clean(qk), qv]))\n",
    "        \n",
    "#     # Third: Make sure the statement is qualifier \n",
    "#     output_qualifier = set()\n",
    "#     for statement, qkv_pairs in qualifier.items():    \n",
    "#         if len(qkv_pairs) > 0:\n",
    "#             new_qkv_pairs = sorted(qkv_pairs)\n",
    "#             new_qkv_list = []\n",
    "#             for pair in new_qkv_pairs:\n",
    "#                 new_qkv_list += list(pair)\n",
    "#             output_qualifier.add(tuple(list(statement) + new_qkv_list))\n",
    "\n",
    "#     output_qualifier = sorted(output_qualifier)\n",
    "\n",
    "#     if output:\n",
    "#         str_q = [\",\".join(q)+'\\n' for q in output_qualifier]\n",
    "#         with open(file_name, 'w', encoding='utf-8') as f:\n",
    "#             f.writelines(str_q)\n",
    "\n",
    "#     return output_qualifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version that will have same sort\n",
    "def get_qualifier_relational_clean_fullname_combine(kb_json, output=False, file_name='kb_q_r_clean_fullname_combine.txt'):\n",
    "    qualifier = dict()\n",
    "    kb = json.load(open(kb_json))\n",
    "    for i in kb['entities']:\n",
    "        fullname = kb['entities'][i]['name']\n",
    "        for rel_dict in kb['entities'][i]['relations']:\n",
    "            # First: add fact key, also called triple pairs\n",
    "            statement = None\n",
    "            if rel_dict['direction'] == 'forward':\n",
    "                statement = (string_clean(fullname), string_clean(rel_dict['predicate']), string_clean(find_name(kb, rel_dict['object'])))\n",
    "            elif  rel_dict['direction'] == 'backward':\n",
    "                statement = (string_clean(find_name(kb, rel_dict['object'])), string_clean(rel_dict['predicate']), string_clean(fullname))\n",
    "            \n",
    "            if not statement in qualifier: \n",
    "                qualifier[statement] = dict()\n",
    "\n",
    "            for qk, qvs in rel_dict['qualifiers'].items():                \n",
    "                # Second add qk - qv pairs, for qv that have more than one instance, seperate to single qk - qv pairs\n",
    "                new_qvs = []\n",
    "                for qv in qvs:\n",
    "                    if qv['type'] == 'string':\n",
    "                        new_qvs.append(string_clean(qv['value']))\n",
    "                        \n",
    "                if len(new_qvs) != 0:\n",
    "                    for qv in new_qvs:\n",
    "                        if string_clean(qk) not in qualifier[statement]:\n",
    "                            qualifier[statement][string_clean(qk)] = [qv]\n",
    "                        else:\n",
    "                            if qv not in qualifier[statement][string_clean(qk)]:\n",
    "                                qualifier[statement][string_clean(qk)] += [qv]\n",
    "        \n",
    "    # Third: Make sure the statement is qualifier \n",
    "    output_qualifier = set()\n",
    "    for statement, qkv_pairs in qualifier.items():    \n",
    "        if len(qkv_pairs) > 0:\n",
    "            new_qkv_list = []\n",
    "            for qk in qkv_pairs:\n",
    "                for qv in qkv_pairs[qk]:\n",
    "                    new_qkv_list += [qk, qv]\n",
    "            output_qualifier.add(tuple(list(statement) + new_qkv_list))\n",
    "\n",
    "    # output_qualifier = sorted(output_qualifier)\n",
    "\n",
    "    if output:\n",
    "        str_q = [\",\".join(q)+'\\n' for q in output_qualifier]\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            f.writelines(str_q)\n",
    "\n",
    "    return output_qualifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = get_qualifier_relational_clean_fullname_combine(kb_json, output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({5: 14377, 7: 6128, 9: 1395, 11: 851, 13: 327, 15: 220, 19: 114, 23: 59, 17: 30, 27: 21, 31: 19, 35: 11, 39: 11, 21: 11, 43: 11, 25: 9, 37: 9, 33: 8, 29: 7, 47: 7, 41: 5, 63: 4, 51: 4, 45: 3, 49: 2, 109: 1, 119: 1, 53: 1, 133: 1, 153: 1, 67: 1, 137: 1, 55: 1, 83: 1, 129: 1, 57: 1, 71: 1, 65: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter()\n",
    "for st in q:\n",
    "    c.update([len(st)])\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relational_clean_fullname_combine(kb_json, output=False, file_name='kb_r_clean_fullname_combine.txt'):\n",
    "    qualifier = dict()\n",
    "    kb = json.load(open(kb_json))\n",
    "    for i in kb['entities']:\n",
    "        fullname = kb['entities'][i]['name']\n",
    "        \n",
    "        # For instance of\n",
    "        for concept_id in kb['entities'][i]['instanceOf']:\n",
    "            statement = (string_clean(fullname), 'instance of', string_clean(find_name(kb, concept_id)))\n",
    "            if not statement in qualifier: \n",
    "                qualifier[statement] = dict()\n",
    "\n",
    "        for rel_dict in kb['entities'][i]['relations']:\n",
    "            # First: add fact key, also called triple pairs\n",
    "            statement = None\n",
    "            if rel_dict['direction'] == 'forward':\n",
    "                statement = (string_clean(fullname), string_clean(rel_dict['predicate']), string_clean(find_name(kb, rel_dict['object'])))\n",
    "            elif  rel_dict['direction'] == 'backward':\n",
    "                statement = (string_clean(find_name(kb, rel_dict['object'])), string_clean(rel_dict['predicate']), string_clean(fullname))\n",
    "            \n",
    "            if not statement in qualifier: \n",
    "                qualifier[statement] = dict()\n",
    "\n",
    "            for qk, qvs in rel_dict['qualifiers'].items():                \n",
    "                # Second add qk - qv pairs, for qv that have more than one instance, seperate to single qk - qv pairs\n",
    "                new_qvs = []\n",
    "                for qv in qvs:\n",
    "                    if qv['type'] == 'string':\n",
    "                        new_qvs.append(string_clean(qv['value']))\n",
    "                        \n",
    "                if len(new_qvs) != 0:\n",
    "                    for qv in new_qvs:\n",
    "                        if string_clean(qk) not in qualifier[statement]:\n",
    "                            qualifier[statement][string_clean(qk)] = [qv]\n",
    "                        else:\n",
    "                            if qv not in qualifier[statement][string_clean(qk)]:\n",
    "                                qualifier[statement][string_clean(qk)] += [qv]\n",
    "\n",
    "    # Third: Add statement\n",
    "    output_qualifier = set()\n",
    "    for statement, qkv_pairs in qualifier.items():    \n",
    "        new_qkv_list = []\n",
    "        for qk in qkv_pairs:\n",
    "            for qv in qkv_pairs[qk]:\n",
    "                new_qkv_list += [qk, qv]\n",
    "        output_qualifier.add(tuple(list(statement) + new_qkv_list))\n",
    "    \n",
    "    # output_qualifier = sorted(output_qualifier)\n",
    "    \n",
    "    if output:\n",
    "        str_q = [\",\".join(q)+'\\n' for q in output_qualifier]\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            f.writelines(str_q)\n",
    "    \n",
    "    return output_qualifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = get_relational_clean_fullname_combine(kb_json, output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes_clean_fullname(kb_json, output=False, file_name='kb_a_clean_fullname_combine.txt'):\n",
    "    qualifier = dict()\n",
    "    kb = json.load(open(kb_json))\n",
    "    for i in kb['entities']:\n",
    "        fullname = kb['entities'][i]['name']\n",
    "\n",
    "        # For attribute\n",
    "        for att_dict in kb['entities'][i]['attributes']:\n",
    "            # First: if it is literal, ignore it\n",
    "            if att_dict['value']['type'] != 'string':\n",
    "                continue\n",
    "            else:\n",
    "                # Second: add attributes\n",
    "                statement = (string_clean(fullname), string_clean(att_dict['key']), string_clean(att_dict['value']['value']))\n",
    "            \n",
    "            if not statement in qualifier: \n",
    "                qualifier[statement] = dict()\n",
    "\n",
    "            for qk, qvs in att_dict['qualifiers'].items():                \n",
    "                # Second add qk - qv pairs, for qv that have more than one instance, seperate to single qk - qv pairs\n",
    "                new_qvs = []\n",
    "                for qv in qvs:\n",
    "                    if qv['type'] == 'string':\n",
    "                        new_qvs.append(string_clean(qv['value']))\n",
    "                        \n",
    "                if len(new_qvs) != 0:\n",
    "                    for qv in new_qvs:\n",
    "                        if string_clean(qk) not in qualifier[statement]:\n",
    "                            qualifier[statement][string_clean(qk)] = [qv]\n",
    "                        else:\n",
    "                            if qv not in qualifier[statement][string_clean(qk)]:\n",
    "                                qualifier[statement][string_clean(qk)] += [qv]\n",
    "            \n",
    "    # Third: Add statement\n",
    "    output_qualifier = set()\n",
    "    for statement, qkv_pairs in qualifier.items():    \n",
    "        new_qkv_list = []\n",
    "        for qk in qkv_pairs:\n",
    "            for qv in qkv_pairs[qk]:\n",
    "                new_qkv_list += [qk, qv]\n",
    "        output_qualifier.add(tuple(list(statement) + new_qkv_list))\n",
    "    \n",
    "    # output_qualifier = sorted(output_qualifier)\n",
    "    \n",
    "    if output:\n",
    "        str_q = [\",\".join(q)+'\\n' for q in output_qualifier]\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            f.writelines(str_q)\n",
    "    \n",
    "    return output_qualifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = get_attributes_clean_fullname(kb_json, output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_clean_fullname(kb_json, output=False, file_name='kb_all_clean_fullname_combine.txt'):\n",
    "    qualifier = dict()\n",
    "    kb = json.load(open(kb_json))\n",
    "    for i in kb['entities']:\n",
    "        fullname = kb['entities'][i]['name']\n",
    "        \n",
    "        # For instance of\n",
    "        for concept_id in kb['entities'][i]['instanceOf']:\n",
    "            statement = (string_clean(fullname), 'instance of', string_clean(find_name(kb, concept_id)))\n",
    "            if not statement in qualifier: \n",
    "                qualifier[statement] = dict()\n",
    "\n",
    "        # For relation\n",
    "        for rel_dict in kb['entities'][i]['relations']:\n",
    "            # First: add fact key, also called triple pairs\n",
    "            statement = None\n",
    "            if rel_dict['direction'] == 'forward':\n",
    "                statement = (string_clean(fullname), string_clean(rel_dict['predicate']), string_clean(find_name(kb, rel_dict['object'])))\n",
    "            elif  rel_dict['direction'] == 'backward':\n",
    "                statement = (string_clean(find_name(kb, rel_dict['object'])), string_clean(rel_dict['predicate']), string_clean(fullname))\n",
    "            \n",
    "            if not statement in qualifier: \n",
    "                qualifier[statement] = dict()\n",
    "\n",
    "            for qk, qvs in rel_dict['qualifiers'].items():                \n",
    "                # Second add qk - qv pairs, for qv that have more than one instance, seperate to single qk - qv pairs\n",
    "                new_qvs = []\n",
    "                for qv in qvs:\n",
    "                    if qv['type'] == 'string':\n",
    "                        new_qvs.append(string_clean(qv['value']))\n",
    "                        \n",
    "                if len(new_qvs) != 0:\n",
    "                    for qv in new_qvs:\n",
    "                        if string_clean(qk) not in qualifier[statement]:\n",
    "                            qualifier[statement][string_clean(qk)] = [qv]\n",
    "                        else:\n",
    "                            if qv not in qualifier[statement][string_clean(qk)]:\n",
    "                                qualifier[statement][string_clean(qk)] += [qv]\n",
    "\n",
    "        # For attribute\n",
    "        for att_dict in kb['entities'][i]['attributes']:\n",
    "            # First: if it is literal, ignore it\n",
    "            if att_dict['value']['type'] != 'string':\n",
    "                continue\n",
    "            else:\n",
    "                # Second: add attributes\n",
    "                statement = (string_clean(fullname), string_clean(att_dict['key']), string_clean(att_dict['value']['value']))\n",
    "            \n",
    "            if not statement in qualifier: \n",
    "                qualifier[statement] = dict()\n",
    "\n",
    "            for qk, qvs in att_dict['qualifiers'].items():                \n",
    "                # Second add qk - qv pairs, for qv that have more than one instance, seperate to single qk - qv pairs\n",
    "                new_qvs = []\n",
    "                for qv in qvs:\n",
    "                    if qv['type'] == 'string':\n",
    "                        new_qvs.append(string_clean(qv['value']))\n",
    "                        \n",
    "                if len(new_qvs) != 0:\n",
    "                    for qv in new_qvs:\n",
    "                        if string_clean(qk) not in qualifier[statement]:\n",
    "                            qualifier[statement][string_clean(qk)] = [qv]\n",
    "                        else:\n",
    "                            if qv not in qualifier[statement][string_clean(qk)]:\n",
    "                                qualifier[statement][string_clean(qk)] += [qv]\n",
    "\n",
    "    # Third: Add statement\n",
    "    output_qualifier = set()\n",
    "    for statement, qkv_pairs in qualifier.items():    \n",
    "        new_qkv_list = []\n",
    "        for qk in qkv_pairs:\n",
    "            for qv in qkv_pairs[qk]:\n",
    "                new_qkv_list += [qk, qv]\n",
    "        output_qualifier.add(tuple(list(statement) + new_qkv_list))\n",
    "    \n",
    "    # output_qualifier = sorted(output_qualifier)\n",
    "    \n",
    "    if output:\n",
    "        str_q = [\",\".join(q)+'\\n' for q in output_qualifier]\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            f.writelines(str_q)\n",
    "    \n",
    "    return output_qualifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = get_all_clean_fullname(kb_json, output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(s: set, split: list=[0.85, 0.15]):\n",
    "    str_l = [\",\".join(q)+'\\n' for q in s]\n",
    "    str_l = np.array(str_l)\n",
    "    length = len(str_l)\n",
    "    permutation = np.random.permutation(length).reshape(-1)\n",
    "    trn_length = np.round(length * split[0]).astype(int)\n",
    "    # vld_length = np.round(length * split[1])\n",
    "    tst_length = np.round(length * split[1]).astype(int)\n",
    "    # assert (trn_length + vld_length + tst_length) == length\n",
    "    assert (trn_length + tst_length) == length\n",
    "    trn = str_l[permutation[0:trn_length]]\n",
    "    # vld = str_l[permutation[trn_length:trn_length+vld_length]]\n",
    "    # tst = str_l[permutation[trn_length+vld_length:length]]\n",
    "    tst = str_l[permutation[trn_length:length]]\n",
    "\n",
    "    with open(\"train.txt\", 'w')as f:\n",
    "        f.writelines(trn)\n",
    "    with open(\"test.txt\", 'w')  as f:\n",
    "        f.writelines(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_test(s: set, split: int=0.01):\n",
    "    str_l = [\",\".join(q)+'\\n' for q in s]\n",
    "    str_l = np.array(str_l)\n",
    "    length = len(str_l)\n",
    "    permutation = np.random.permutation(length).reshape(-1)\n",
    "    trn_length = np.round(length).astype(int)\n",
    "    tst_length = np.round(length * split).astype(int)\n",
    "    trn = str_l[permutation]\n",
    "    tst = str_l[permutation[:tst_length]]\n",
    "\n",
    "    with open(\"train.txt\", 'w')as f:\n",
    "        f.writelines(trn)\n",
    "    with open(\"test.txt\", 'w')  as f:\n",
    "        f.writelines(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_fake_test(q)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3818dc3fdeae4bde46e4525c36b3f7717722ff31b1f92b9c8879a52c17e23e4e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
